{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying all the imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool, tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from dotenv import load_dotenv\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser, ListOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GenAIProject\\GenAIGroupProject\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from vector_database_kh import create_milvus_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    "    utility,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GenAIProject\\GenAIGroupProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index and embeddings have been successfully loaded from data/annual_filings_model_state.pkl\n",
      "Index and embeddings have been successfully inserted into your Milvus Collection\n"
     ]
    }
   ],
   "source": [
    "vector_store = create_milvus_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(host=\"localhost\", port=\"19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GenAIProject\\GenAIGroupProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_qn = \"How many iphones were sold in 2021\"\n",
    "embedded_query_qn = model.encode(query_qn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id: 1935, distance: 0.6662631034851074, entity: {}', 'id: 2145, distance: 0.7552067041397095, entity: {}', 'id: 1700, distance: 0.7903969883918762, entity: {}', 'id: 397, distance: 0.7917543649673462, entity: {}', 'id: 1978, distance: 0.7963277101516724, entity: {}']\n",
      "id: 1935, distance: 0.6662631034851074, entity: {}\n",
      "1935\n"
     ]
    }
   ],
   "source": [
    "search_params = {\n",
    "    \"metric_type\": \"L2\"\n",
    "}\n",
    "\n",
    "result = vector_store.search(\n",
    "    [embedded_query_qn.tolist()], # a single query must still be enclosed in a list\n",
    "    \"embedding\",\n",
    "    search_params,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "for hit in result:\n",
    "    print(hit)\n",
    "    print(hit[0])\n",
    "    print(hit[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pk': 1935, 'embedding': [-0.06728231, -0.09216395, 0.04787752, -0.03930773, 0.019780798, 0.06657909, 0.012856519, 0.020835219, 0.017616091, 0.018569017, 0.075725965, 0.033424772, 0.025746675, -0.017526561, 0.07075583, -0.04813127, 0.052598156, -0.070672676, -0.07139801, -0.009615908, 0.050134078, 0.06436616, 0.008036388, 0.04388648, 0.05663751, 0.032957144, -0.10109895, -0.061987128, -0.070719786, -0.008989388, -0.0072938926, 0.053331256, 0.035424292, 0.08004637, -0.0049086073, -0.15163846, -0.029077401, 0.010319345, 0.0041560233, -0.020652303, -0.029792583, -0.08916836, -0.049034085, 0.104900874, 0.053027555, 0.0041458793, -0.07386805, 0.022767719, -0.022616688, 0.092046306, -0.002511514, 0.046232514, -0.009588766, 0.07673581, -0.0008623279, 0.009017158, -0.06038827, 0.019698802, 0.064244375, 0.0404921, 0.08551747, -0.066516764, 0.0008964158, 0.00067777035, 0.024973558, -0.02927507, -0.052622207, -0.041493125, -0.01517896, 0.005326369, -0.01629912, -0.018979156, 0.05208672, 0.039057337, 0.009198261, 0.011416213, 0.08249938, -0.032259073, -0.030279284, -0.039902586, -0.030760761, 0.0058837333, 0.028263586, -0.0039508, -0.011791161, -0.007270629, -0.037948504, 0.014488933, -0.024150034, -0.078713, -0.030120013, 0.06269451, -0.039854836, -0.023831483, -0.044427115, -0.021104889, -0.023918053, -0.035516683, 0.01927797, 0.038662888, 0.042433515, 0.0191977, 0.012608829, -0.011114413, -0.011620768, -0.09255337, -0.0028327277, 0.007745484, -0.0010148457, 0.10380965, -0.060966786, -0.038322814, -0.03945383, -0.05567529, 0.0041064364, 0.03023272, -0.018015917, 0.10299855, 0.16535676, -0.02639549, 0.000431443, 0.06505134, 0.042670365, -0.05884404, -0.1266235, -0.005222778, -0.059931666, 1.7896038e-32, -0.09399871, -0.011063965, -0.012987526, -0.034549434, -0.040655345, -0.040674098, 0.060337633, 0.10124192, 0.06649527, 0.0609954, -0.09890731, 0.024297431, -0.033615094, 0.030842451, 0.10644115, -0.032198995, -0.04495718, -0.007524782, 0.05168022, 0.0009393429, -0.0065017915, -0.09052118, -0.02531869, 0.06369589, 0.0047100983, -0.050396793, 0.03290418, -0.012002894, 0.04629856, 0.003546569, 0.033958666, 0.0028345962, 0.055737056, -0.11735336, -0.07132478, -0.0313031, 0.016439678, -0.026328614, 0.06639046, -0.018927407, -0.06610856, -0.0031371056, -0.063586675, -0.05341439, -0.00029639187, 0.027642427, 0.014948402, 0.0093501005, 0.0071403626, 0.03431658, -0.058388706, -0.01849457, -0.066773586, -0.050295353, -0.042830717, -0.03602764, -0.0051884525, -0.08349108, 0.00021279683, 0.041456573, -0.027463062, -0.05266407, -0.004767377, -0.06877579, -0.05041924, 0.050874367, 0.056135844, -0.07384613, -0.04004835, 0.119857535, -0.02516556, 0.025415797, 0.0034581518, -0.051848788, 0.053446636, 0.0038837583, -0.016618147, -0.027434723, -0.01665087, -0.041406613, -0.052204143, 0.059367787, 0.079837084, 0.047865193, 0.019454453, -0.03934697, -0.0061420654, -0.05143794, 0.008795827, 0.023574416, -0.04262483, -0.026307972, -0.061724316, 0.019898664, -0.017132364, -1.7315324e-32, -0.03024313, 0.04492624, -0.07133708, 0.009351351, 0.009856058, -0.04636085, -0.017894069, 0.07324038, 0.042401806, -0.027370801, 0.027289148, 0.020424817, -0.01551081, 0.09192216, -0.0629771, -0.00033200654, -0.10029663, -0.108701095, 0.03950506, 0.0372856, -0.0246317, 0.09848932, 0.08134406, 0.012483711, -0.048255056, 0.011393906, -0.04747083, 0.045311578, 0.08057642, -0.027354075, -0.0074134604, -0.14554106, -0.01798355, 0.07025332, 0.06179682, -0.019409863, 0.041574147, -0.02079853, -0.0037770567, -0.028337108, 0.08777338, -0.04983958, 0.022063754, 0.0163066, 0.03681827, -0.07917191, -0.017689943, 0.024307031, 0.0446634, -0.030632982, 0.018590221, 0.091195874, -0.07571021, 0.051628582, -0.11075304, 0.065784805, 0.07730389, -0.0006032552, -0.020380896, -0.020543862, 0.051137097, 0.00659405, -0.0137503175, -0.016334152, 0.061155584, -0.024774166, -0.028329395, -0.011669132, 0.014053703, 0.013585739, -0.026661133, -0.047261897, -0.05439546, -0.09152683, -0.070141755, -0.042109396, 0.0015790155, -0.014552352, -0.019438803, -0.03306666, 0.028385475, 0.096041195, 0.04177758, 0.01095427, -0.025663793, -0.021377604, 0.056688234, -0.00047939754, -0.04353903, 0.09236958, -0.07996135, -0.019063614, -0.07593543, 0.076095216, -0.05065574, -6.306147e-08, 0.05992913, 0.07644148, 0.030266585, 0.009884603, 0.033486158, -0.0063955444, 0.017173352, 0.07056743, 0.104781054, -0.007612648, 0.0066986303, -0.028150102, -0.093467414, 0.069834635, -0.0013293222, 0.05396839, -0.09493028, 0.023251897, 0.022572566, -0.03004149, 0.028088301, 0.022445513, -0.033853587, -0.0071918964, -0.019197498, 0.001033548, 0.020912023, 0.07876388, 0.01970557, 0.030046219, 0.019092187, -0.0040458827, 0.046965204, -0.06450905, 0.0052028135, -0.048332084, -0.062768325, -0.04363994, -0.0025233184, 0.02538943, -0.020189153, -0.095659845, -0.061364762, 0.056663044, 0.07543956, -0.1384394, -0.039227977, -0.09399546, 0.060762525, -0.05348548, -0.06540532, -0.02719507, 0.02679617, -0.060663015, 0.012141989, -0.018774375, 0.0014037435, -0.03133501, 0.014966167, 0.093712285, 0.07501831, -0.08601993, 0.040313005, 0.03895198]}]\n"
     ]
    }
   ],
   "source": [
    "search_result = vector_store.query(expr=f\"pk == {1935}\", output_fields=[\"*\"])\n",
    "\n",
    "# Print the search result\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decoded_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(search_result)\n",
      "File \u001b[1;32mc:\\GenAIProject\\GenAIGroupProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "decoded_result = model.decode(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many iphones were sold in 2021\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_qn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mloc[hit[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mid]\u001b[38;5;241m.\u001b[39manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query_qn}\")\n",
    "print(f\"Response: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def get_numbers(query: str):\n",
    "    llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), model='gpt-3.5-turbo', temperature=0.2)\n",
    "    template = \"\"\"\n",
    "    You are smart agent that can idenify all the client ids to solve the query. \n",
    "    If no start point for a client is mentioned, assume client 1 is the first client.\n",
    "    id. From the query, return all the ids that the query refers to as a list of integers.\n",
    "    {format_instructions}\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    class Numbers(BaseModel):\n",
    "        numbers: str = Field(description=\"should say 'the numbers extracted from the query'\")\n",
    "        relevant_numbers: list = Field(description=\"should be a list of all the relevant numbers the llm has picked up\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Numbers)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = ['query'],\n",
    "        template=template,\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    ans = chain.invoke(query)\n",
    "    return ans['relevant_numbers']\n",
    "        \n",
    "\n",
    "@tool\n",
    "def search_tool(query : str):\n",
    "    \"\"\" \n",
    "    Takes a user query and performs an online search to provide\n",
    "    an answer to that query.\n",
    "    \"\"\"\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    answer = search.run(query)\n",
    "    return answer\n",
    "\n",
    "@tool\n",
    "def create_embeddings(query : str):\n",
    "    \"\"\"\n",
    "    Uses the content from the file path to create embeddings on the inputted data. Using the query\n",
    "    provided, perform a similarity search on the query and return the output. If you can not provide a response,\n",
    "    say I don't know rather than providing an incorrect response.\n",
    "    \"\"\"\n",
    "\n",
    "    #file_path = r'C:\\GenAI\\Final Project\\GenAIGroupProject\\apple.txt'\n",
    "    file_path = r'./apple.txt'\n",
    "    file_path = file_path.strip()\n",
    "    loader = TextLoader(file_path, encoding= 'utf-8')\n",
    "    document = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunk = text_splitter.split_documents(documents=document)\n",
    "    embeddings = OpenAIEmbeddings(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    vectorstore = FAISS.from_documents(chunk, embeddings)\n",
    "\n",
    "    docs = vectorstore.similarity_search(query)\n",
    "    return docs[0].page_content\n",
    "\n",
    "\n",
    "@tool\n",
    "def query_database(query : str):\n",
    "    \"\"\"\n",
    "    This function queries the SQL database based on the query that the user inputs.\n",
    "    This will provide insight into the current client(s) stock portfolio from the portfolio table in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), model='gpt-3.5-turbo', temperature=0)\n",
    "    db = SQLDatabase.from_uri('sqlite:///portfolio_allocations.db')\n",
    "    agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)\n",
    "    result = agent_executor.invoke(query)\n",
    "    \n",
    "\n",
    "    client = get_numbers(query)\n",
    "    for i in client:\n",
    "        response_schemas = [\n",
    "        ResponseSchema(name=f'Client {i}', description=f\"A dictionary containing the stock tickers and their percentage allocations for each client {i}.\")\n",
    "        ]\n",
    "      \n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "    template=\"create a new key, value pair for every client mentioned in the question.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    ans = chain.invoke({\"question\": result})\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "@tool\n",
    "def portfolio_allocation(query : str):\n",
    "    \"\"\"\n",
    "    Based on the allocation stratergy provided as a query, context on the comapany's recent financials from their 10-k reports\n",
    "    and their up-to-date status from an online search, change the allocation in the\n",
    "    client stock portfolios in the SQL database.\n",
    "    \"\"\"\n",
    " \n",
    "    template = \"\"\"\n",
    "    You are a helpful assistant who is going to allocate different weights based on context and the invest stratergy that user wants.\n",
    "    In order to retreieve the context, use the search_tool and the create_embedding tool to search 'Tell me about the fininical siutuation of\n",
    "    AAPL, MSFT, NVIDIA'. Combining this with the allocation stratergy that the user picks, redistribute the allocation percentages. Give exact figures \n",
    "    as to how this is to be used for each stock. You should also give a detailed explanation as to why you came up with these figures. Use the context\n",
    "    to provide statistics in order to boost your reasoning. Make sure the reasoning is also returned in the final answwer.\n",
    "\n",
    "    Query : {query}\n",
    "    {answer}\n",
    "    \"\"\"\n",
    "   \n",
    "    examples = [\n",
    "    {\n",
    "        'query': 'Reallocte the portfolio for client 1 with a balanced strategy',\n",
    "        'answer': 'The stock allocations for client_id 1 equal across all stocks, totalling 100.'\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'query': 'Reallocte the portfolio for client 1 with a risk-based strategy',\n",
    "        'answer': \"\"\"\n",
    "                        Allocating more to stocks with lower perceived risk. \n",
    "                        From my 10k data and online search, Company 1 has strong financials\n",
    "                        and minimal risk but Comapany 2 has more risk. As a result, the stock\n",
    "                        allocation fror client_id 1 would have a higher percentage for Company 1\n",
    "                        and a lower percentage for Company 2. Company 3 would make up the rest of the portfolio.\n",
    "                        \"\"\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'query': 'Reallocte the portfolio for client 1 with a return-based strategy',\n",
    "        'answer' : \"\"\"\n",
    "                        Allocating more to stocks with higher expected returns.\n",
    "                        From my 10k data and online search, Company 3 has higher expected growth rates and returns\n",
    "                        whereas Company 2 has a declining industry position and poor management quality. As a result, the stock\n",
    "                        allocation fror client_id 1 would have a high percentage for Company 3\n",
    "                        and a low percentage for Company 2 and Company 1 would make up the rest of the portfolio.\n",
    "                        \"\"\"\n",
    "    }\n",
    "    ]\n",
    "    c1 = create_embeddings(\"what is the current financial outlook of AAPL, MSFT, NVIDIA\")\n",
    "    c2 = search_tool(\"what is the current financial outlook of AAPL, MSFT, NVIDIA\")\n",
    "\n",
    "    context = c1 + c2\n",
    "\n",
    "    example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=template\n",
    "    )\n",
    "\n",
    "    prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Query: {input}, Context: {context}\",\n",
    "    input_variables=[\"input\", \"context\"],\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), model='gpt-3.5-turbo', temperature=0.2)\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    #print(f\"context: {context}\")\n",
    "    ans = chain.invoke({'input' : query, 'context':context})\n",
    "    \n",
    "    query_database(f\"set the stock allocations for the client and percentages from {ans.content} provide the sql query to do so and execute it on the database\")\n",
    "    \n",
    "    return ans.content\n",
    "\n",
    "    \n",
    "tools = [Tool(name=\"search_tool\", func=search_tool, description=\"Tool for performing online search operations\"),\n",
    "         Tool(name=\"create_embeddings\", func=create_embeddings, description=\"Uses the content from the file path to create embeddings on the inputted data. Using the query provided, perform a similarity search on the query and return the output. If you can not provide a response,say I don't know rather than providing an incorrect response.\"),\n",
    "         Tool(name='query_database', func=query_database, description= \"This function queries the SQL database based on the query that the user provides to provide insight into the current client stock portfolio\"),\n",
    "         Tool(name='portfolio_allocation', func=portfolio_allocation, description='This function uses context from the vector database and online searches as well as the investment stratergy the user specifies to determine the stock allocation splits')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GenAIProject\\GenAIGroupProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the search_tool to find the current Apple stock price.\n",
      "\n",
      "Action: search_tool\n",
      "Action Input: \"Apple stock price today\"\u001b[0m\u001b[36;1m\u001b[1;3mGet the latest Apple (AAPL) stock price, quote, news and history on Nasdaq. See the bid, ask, volume, 52 week range, earnings surprises, PEG ratio and more for AAPL. Get the latest stock price, change, volume and historical data for Apple (NASDAQ: AAPL) as of May 6, 2024. See the stock performance and charts for the past year and compare with other dates. Stock analysis for Apple Inc (AAPL:NASDAQ GS) including stock price, stock chart, company news, key statistics, fundamentals and company profile. View Apple Inc. AAPL stock quote prices, financial information, real-time forecasts, and company news from CNN. View Apple Inc AAPL investment & stock information. Get the latest Apple Inc AAPL detailed stock quotes, stock data, Real-Time ECN, charts, stats and more. Apple - AAPL - Stock Price Today - Zacks\u001b[0m\u001b[32;1m\u001b[1;3mI should now have the answer to the original question.\n",
      "\n",
      "Final Answer: The current stock price for Apple (AAPL) is $XXX.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The current stock price for Apple (AAPL) is $XXX.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You need to use the tools to find the appropriate action to take. \n",
    "\n",
    "If you are searching for information use the create_emebddings tool FIRST.\n",
    "\n",
    "Only use the query_database and portfolio_allocation tools when the term \"clients\" is mentioned in the query .\n",
    "if only the query_database is used, the output has to be EXACTLY as it is in the function.\n",
    "and if further queries are presented alongside the term \"clients\", then use all tools available.\n",
    "\n",
    "Try to get relevant responses within our internal documents. If you get relevant\n",
    "responses, return them to the user. \n",
    "\n",
    "If you do not get relevent results from the embeddings, use the search_tool to get responses from the internet.\n",
    "\n",
    "If you still can't find an answer from an online search, \n",
    "please do not make up an answer and return 'Sorry I cannot answer your query' instead.\n",
    "\n",
    "If performing portfolio_allocation, use the tools \"search_tool\" and \"create_embeddings\" \n",
    "to get context around AAPL, MSFT, NVIDIA's financial situation. \n",
    "Use a query with these tools that states: \"Tell me about the company's financial situation\".\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Question: the input question you must answer\n",
    "\n",
    "Thought: you should always think about what to do\n",
    "\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "\n",
    "Action Input: the input to the action\n",
    "\n",
    "Observation: the result of the action\n",
    "\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "\n",
    "If your thought before the final answer includes the output from the portfolio_allocation tool, give the response by including ACTUAL percentages totalling to 100. \n",
    "\n",
    "If your thought before the final answer uses the query_database tool, treat the tool as a function and return a dictionary output as the final answer.\n",
    "\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define the query and create the prompt\n",
    "#query = 'return the values for client_id 1 in the sql database and also tell me the current apple stock price'\n",
    "query = 'Tell me the todays Apple stock price'\n",
    "#query = 'Return me the stock allocation for client 5'\n",
    "# query = 'Return me the stock allocation for client 8'\n",
    "#query = 'Return me the stock allocation for every client'\n",
    "#query = 'give me all the stock allocations from all clients up to client 3'\n",
    "#query = \"The Company continues to develop new technologies to enhance existing products and services and to expand the range of its offerings through research and development RD licensing of intellectual property and acquisition of thirdparty businesses and technology\"\n",
    "# query = \"Give me a sentence from the apple 10-k report\"\n",
    "#query = \"what is the net sales of iPhones in 2021\"\n",
    "#query =  \"add a new client to the database with random stock allocations, provide the sql query to do so and execute it\"\n",
    "#query =  \"set the stock allocations for client_id 1 to 5,5,90. provide the sql query to do so and execute it on the database\" #remeber to pull numbers from another query into this framework\n",
    "#query = 'give me all the stock allocations from all clients up to client 3'\n",
    "# query = 'give me all the stock allocations for all clients up to client 3'\n",
    "# query = 'Reallocate the stocks for client 3 using a balanced stratergy'\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['input','tools', 'agent_scratchpad', 'tool_names'], template=template)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), model='gpt-3.5-turbo', temperature=0.2)\n",
    "\n",
    "\n",
    "# Create the agent using the language model and the toolset\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "# Execute the agent with the given input\n",
    "response = agent_executor.invoke({'input': query, 'tools' : tools, 'tool_names' : [tool.name for tool in tools]})\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
