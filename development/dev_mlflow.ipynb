{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: ML Flow\n",
    "\n",
    "File for developing the mlflow code for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Use the below lines if any dependencies are missing.\n",
    "# ! python -m pip install uv\n",
    "# ! python -m uv pip install langchain_openai mlflow load_dotenv langchain pandas langchain_community\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('\\\\'.join(os.getcwd().split('\\\\')[:-1])))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from ml_flow import mlflow_server\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we run the ML-Flow server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully running ML-Flow server. The server will terminate at the end of runtime.\n"
     ]
    }
   ],
   "source": [
    "server_process = mlflow_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a dummy LLM which will answer simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP EXAMPLE LLM ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "example_llm = ChatOpenAI(model_name='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['inputs', 'context'],\n",
    "                                template=(\"You're a investment manager. Using the context provided, \"\n",
    "                                          + \"reply to the question below to the best of your ability:\\n\"\n",
    "                                          + \"Question:\\n{inputs}\\nContext:\\n{context}\"))\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def _get_content(model_return):\n",
    "    return model_return.content\n",
    "\n",
    "get_content = RunnableLambda(_get_content)\n",
    "\n",
    "example_model = example_prompt | example_llm | get_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then create an evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "\n",
       "                                             targets  \n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) +'/data/'\n",
    "file_name = 'Evaluation Dataset.csv'\n",
    "\n",
    "file_path = data_folder_path + file_name\n",
    "eval_set = pd.read_csv(file_path)\n",
    "\n",
    "display(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['inputs', 'context', 'targets'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Client 1 has £14,000 worth of NVDA shares (70% of 20 shares at £1000 per share) and £1,140 worth of APPL shares (30% of 20 shares at £190 per share). Therefore, in total, Client 1 has £15,140 in shares.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model.invoke({'inputs': eval_set['inputs'][0], 'context': eval_set['context'][0]})#.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define an LLM-as-a-judge metric, and give it an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading prompt is:\n",
      "\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "EvaluationMetric(name=Answer_Quality, greater_is_better=True, long_name=Answer_Quality, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called Answer_Quality based on the input and output.\n",
      "A definition of Answer_Quality and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer Quality is a measure of the accuracy of the answer.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is the best stock that client 2 currently owns?\n",
      "\n",
      "Example Output:\n",
      "The best performing stock owned by client 2 is NVDA, which has seen a 400% increase in value in the last 10 months.\n",
      "\n",
      "Example score: 80\n",
      "Example justification: The best performing stock has been identified, and a reason is given for its choosing.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "example = EvaluationExample(input=\"What is the best stock that client 2 currently owns?\",\n",
    "                            output=(r\"The best performing stock owned by client 2 is NVDA, which has seen a 400% \"\n",
    "                                    + \"increase in value in the last 10 months.\"),\n",
    "                            score=80,\n",
    "                            justification=(\"The best performing stock has been identified, and a reason is given for \"\n",
    "                                           + \"its choosing.\"))\n",
    "\n",
    "grading_prompt = (\"Answer Quality: If the answer given does not relate to the question, or if the question is not \"\n",
    "                  + \"answered, we will give a low score. If the question is answered comprehensively we will give a \"\n",
    "                  + \"higher score.\\nScore 0: The question is not answered.\\nScore 20: The question is barely \"\n",
    "                  + \"answered, and the answer is not useful.\\nScore 40: The question is barely answered in basic \"\n",
    "                  + \"terms.\\nScore 80: The question is barely answered correctly and accurately.\\nScore 100: The \"\n",
    "                  + \"question is answer perfectly, and the choices are well reasoned.\")\n",
    "\n",
    "# Make a metric from a Gen AI model.\n",
    "answer_quality = make_genai_metric(name=\"Answer_Quality\",\n",
    "                                   definition=(\"Answer Quality is a measure of the accuracy of the answer.\"),\n",
    "                                   model=\"openai:/gpt-3.5-turbo\",\n",
    "                                   examples=[example],\n",
    "                                   parameters={\"temperature\": 0.0},\n",
    "                                   aggregations=[\"mean\", \"variance\"],\n",
    "                                   greater_is_better=True,\n",
    "                                   grading_prompt=(grading_prompt))\n",
    "\n",
    "print('The grading prompt is:')\n",
    "print('')\n",
    "print(grading_prompt)\n",
    "print('')\n",
    "print(answer_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect to ML-Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import ari_grade_level, toxicity\n",
    "from mlflow.metrics.genai import answer_similarity, faithfulness, relevance, answer_correctness, answer_relevance\n",
    "\n",
    "judge_model = \"openai:/gpt-3.5-turbo\"\n",
    "\n",
    "answer_similarity_metric = answer_similarity(model=judge_model)\n",
    "faithfulness_metric = faithfulness(model=judge_model)\n",
    "relevance_metric = relevance(model=judge_model)\n",
    "answer_correctness_metric = answer_correctness(model=judge_model)\n",
    "answer_relevance_metric = answer_relevance(model=judge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi working chat gpt Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"mlflow_development\")\n",
    "\n",
    "with mlflow.start_run() as run: \n",
    "    _logged_model = mlflow.langchain.log_model(example_model, artifact_path=\"model\")\n",
    "\n",
    "    mlflow.log_param(\"model\", example_model)\n",
    "    results = mlflow.evaluate(_logged_model.model_uri,\n",
    "                              eval_set,\n",
    "                              model_type=\"question-answering\",\n",
    "                              #evaluators=\"default\",\n",
    "                              targets=\"targets\",\n",
    "                              extra_metrics=[relevance_metric, faithfulness_metric, answer_similarity_metric, answer_correctness_metric, answer_relevance_metric],\n",
    "                              #extra_metrics=[answer_quality], # Include our custom metric!\n",
    "                              evaluator_config={'col_mapping': {\"inputs\": \"predictions\"}}\n",
    "                              )\n",
    "\n",
    "    mlflow.log_metrics(results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 37.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>relevance/v1/justification</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>The output accurately reflects the input and c...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly states the distribution o...</td>\n",
       "      <td>3</td>\n",
       "      <td>The output has moderate semantic similarity to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...           66   \n",
       "\n",
       "   relevance/v1/score                         relevance/v1/justification  \\\n",
       "0                   5  The output accurately reflects the input and c...   \n",
       "\n",
       "   faithfulness/v1/score                      faithfulness/v1/justification  \\\n",
       "0                      5  The output correctly states the distribution o...   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           3   \n",
       "\n",
       "                  answer_similarity/v1/justification  \n",
       "0  The output has moderate semantic similarity to...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results.tables['eval_results_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 328.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>relevance/v1/score</th>\n",
       "      <th>relevance/v1/justification</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provides a comprehensive answer to ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly states the distribution o...</td>\n",
       "      <td>3</td>\n",
       "      <td>The output has moderate semantic similarity to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...           66   \n",
       "\n",
       "   relevance/v1/score                         relevance/v1/justification  \\\n",
       "0                   5  The output provides a comprehensive answer to ...   \n",
       "\n",
       "   faithfulness/v1/score                      faithfulness/v1/justification  \\\n",
       "0                      5  The output correctly states the distribution o...   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           3   \n",
       "\n",
       "                  answer_similarity/v1/justification  \n",
       "0  The output has moderate semantic similarity to...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "#output_df['answer'] = [d['content'] for d in output_df['outputs']]\n",
    "\n",
    "# desired_columns = ['inputs', 'context', 'targets'] + [col for col in output_df.columns\n",
    "#                                             if ('score' in col) or ('justification') in col]\n",
    "# output_df = output_df[desired_columns]\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client 1 has £14,000 worth of NVDA shares (70% of 20 shares at £1000 per share) and £5,700 worth of AAPL shares (30% of 20 shares at £190 per share). Therefore, in total, Client 1 has £19,700 in shares.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df['outputs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m experiment \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlflow_development\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fetch all runs for the experiment\u001b[39;00m\n\u001b[0;32m     15\u001b[0m runs \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39msearch_runs(\n\u001b[0;32m     16\u001b[0m     experiment_ids\u001b[38;5;241m=\u001b[39m[experiment\u001b[38;5;241m.\u001b[39mexperiment_id],\n\u001b[0;32m     17\u001b[0m     filter_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Add filter criteria if needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     run_view_type\u001b[38;5;241m=\u001b[39mmlflow\u001b[38;5;241m.\u001b[39mentities\u001b[38;5;241m.\u001b[39mViewType\u001b[38;5;241m.\u001b[39mACTIVE_ONLY  \u001b[38;5;66;03m# You can choose ACTIVE_ONLY, DELETED_ONLY, or ALL\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment_name' is not defined"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Specify the tracking URI (replace with your MLflow server URI)\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080/\")\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = client.get_experiment_by_name('mlflow_development')\n",
    "\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
    "\n",
    "# Fetch all runs for the experiment\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"\",  # Add filter criteria if needed\n",
    "    run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY  # You can choose ACTIVE_ONLY, DELETED_ONLY, or ALL\n",
    ")\n",
    "\n",
    "# Display information about each run\n",
    "for run in runs:\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Parameters: {run.data.params}\")\n",
    "    print(f\"Metrics: {run.data.metrics}\")\n",
    "    print(f\"Tags: {run.data.tags}\")\n",
    "    print(f\"Artifacts: {client.list_artifacts(run.info.run_id)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_experiment(experiment.experiment_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
