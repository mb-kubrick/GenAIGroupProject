{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: ML Flow\n",
    "\n",
    "File for developing the mlflow code for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Use the below lines if any dependencies are missing.\n",
    "# ! python -m pip install uv\n",
    "# ! python -m uv pip install langchain_openai mlflow load_dotenv langchain pandas langchain_community\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('\\\\'.join(os.getcwd().split('\\\\')[:-1])))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from ml_flow import mlflow_server\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we run the ML-Flow server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully running ML-Flow server. The server will terminate at the end of runtime.\n"
     ]
    }
   ],
   "source": [
    "server_process = mlflow_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a dummy LLM which will answer simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP EXAMPLE LLM ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "example_llm = ChatOpenAI(model_name='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['question'],\n",
    "                                template=(\"You're a investment manager. Using your knowledge of investment management, \"\n",
    "                                          + \"reply to the question below to the best of your ability:\\n\"\n",
    "                                          + \"Question:\\n{question}\"))\n",
    "\n",
    "example_model = example_prompt | example_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then create an evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = pd.DataFrame({\"question\": [\"What is the best stock to buy right now?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define an LLM-as-a-judge metric, and give it an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading prompt is:\n",
      "\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "EvaluationMetric(name=Answer_Quality, greater_is_better=True, long_name=Answer_Quality, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called Answer_Quality based on the input and output.\n",
      "A definition of Answer_Quality and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer Quality is a measure of the accuracy of the answer.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is the best stock that client 2 currently owns?\n",
      "\n",
      "Example Output:\n",
      "The best performing stock owned by client 2 is NVDA, which has seen a 400% increase in value in the last 10 months.\n",
      "\n",
      "Example score: 80\n",
      "Example justification: The best performing stock has been identified, and a reason is given for its choosing.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "example = EvaluationExample(input=\"What is the best stock that client 2 currently owns?\",\n",
    "                            output=(r\"The best performing stock owned by client 2 is NVDA, which has seen a 400% \"\n",
    "                                    + \"increase in value in the last 10 months.\"),\n",
    "                            score=80,\n",
    "                            justification=(\"The best performing stock has been identified, and a reason is given for \"\n",
    "                                           + \"its choosing.\"))\n",
    "\n",
    "grading_prompt = (\"Answer Quality: If the answer given does not relate to the question, or if the question is not \"\n",
    "                  + \"answered, we will give a low score. If the question is answered comprehensively we will give a \"\n",
    "                  + \"higher score.\\nScore 0: The question is not answered.\\nScore 20: The question is barely \"\n",
    "                  + \"answered, and the answer is not useful.\\nScore 40: The question is barely answered in basic \"\n",
    "                  + \"terms.\\nScore 80: The question is barely answered correctly and accurately.\\nScore 100: The \"\n",
    "                  + \"question is answer perfectly, and the choices are well reasoned.\")\n",
    "\n",
    "# Make a metric from a Gen AI model.\n",
    "answer_quality = make_genai_metric(name=\"Answer_Quality\",\n",
    "                                   definition=(\"Answer Quality is a measure of the accuracy of the answer.\"),\n",
    "                                   model=\"openai:/gpt-3.5-turbo\",\n",
    "                                   examples=[example],\n",
    "                                   parameters={\"temperature\": 0.0},\n",
    "                                   aggregations=[\"mean\", \"variance\"],\n",
    "                                   greater_is_better=True,\n",
    "                                   grading_prompt=(grading_prompt),\n",
    ")\n",
    "\n",
    "print('The grading prompt is:')\n",
    "print('')\n",
    "print(grading_prompt)\n",
    "print('')\n",
    "print(answer_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect to ML-Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.chat_models.openai:WARNING! stream is not default parameter.\n",
      "                    stream was transferred to model_kwargs.\n",
      "                    Please confirm that stream is what you intended.\n",
      "c:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "WARNING:langchain_community.chat_models.openai:WARNING! stream is not default parameter.\n",
      "                    stream was transferred to model_kwargs.\n",
      "                    Please confirm that stream is what you intended.\n",
      "2024/05/28 17:55:17 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024/05/28 17:55:20 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/05/28 17:55:20 WARNING mlflow.metrics.metric_definitions: Cannot calculate toxicity for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:20 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/28 17:55:20 WARNING mlflow.metrics.metric_definitions: Cannot calculate flesch_kincaid for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:20 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/28 17:55:20 WARNING mlflow.metrics.metric_definitions: Cannot calculate ari for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:20 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/05/28 17:55:20 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "2024/05/28 17:55:21 WARNING mlflow.metrics.metric_definitions: Cannot calculate toxicity for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:21 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/28 17:55:21 WARNING mlflow.metrics.metric_definitions: Cannot calculate flesch_kincaid for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:21 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/28 17:55:21 WARNING mlflow.metrics.metric_definitions: Cannot calculate ari for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/05/28 17:55:21 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/05/28 17:55:21 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"mlflow_development\")\n",
    "\n",
    "with mlflow.start_run() as run: \n",
    "    _logged_model = mlflow.langchain.log_model(example_model, artifact_path=\"model\")\n",
    "\n",
    "    mlflow.log_param(\"model\", example_model)\n",
    "    results = mlflow.evaluate(_logged_model.model_uri,\n",
    "                              eval_set,\n",
    "                              model_type=\"question-answering\",\n",
    "                              extra_metrics=[answer_quality], # Include our custom metric!\n",
    "                              evaluator_config={'col_mapping': {\"inputs\": \"predictions\"}})\n",
    "\n",
    "    mlflow.log_metrics(results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 490.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Answer_Quality/v1/score</th>\n",
       "      <th>Answer_Quality/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the best stock to buy right now?</td>\n",
       "      <td>As an investment manager, I cannot provide spe...</td>\n",
       "      <td>100</td>\n",
       "      <td>The model's response directly addresses the in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question  \\\n",
       "0  What is the best stock to buy right now?   \n",
       "\n",
       "                                              answer  Answer_Quality/v1/score  \\\n",
       "0  As an investment manager, I cannot provide spe...                      100   \n",
       "\n",
       "                     Answer_Quality/v1/justification  \n",
       "0  The model's response directly addresses the in...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "output_df['answer'] = [d['content'] for d in output_df['outputs']]\n",
    "\n",
    "desired_columns = ['question', 'answer'] + [col for col in output_df.columns\n",
    "                                            if ('score' in col) or ('justification') in col]\n",
    "output_df = output_df[desired_columns]\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an investment manager, I cannot provide specific stock recommendations without knowing more about your individual financial situation, risk tolerance, investment goals, and time horizon. It is important to conduct thorough research and analysis before making any investment decisions. I recommend diversifying your portfolio and considering factors such as company fundamentals, industry trends, and market conditions when selecting stocks to buy. It may be beneficial to consult with a financial advisor for personalized investment advice.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df['answer'][0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
