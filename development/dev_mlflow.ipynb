{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: ML Flow\n",
    "\n",
    "File for developing the mlflow code for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Use the below lines if any dependencies are missing.\n",
    "# ! python -m pip install uv\n",
    "# ! python -m uv pip install langchain_openai mlflow load_dotenv langchain pandas langchain_community\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('\\\\'.join(os.getcwd().split('\\\\')[:-1])))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from ml_flow import mlflow_server, create_example_llm, evaluate_llm\n",
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we run the ML-Flow server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_process = mlflow_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a dummy LLM which will answer simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_model = create_example_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then read in an evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                             targets  \n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...  \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) +'/data/'\n",
    "file_name = 'Evaluation Dataset.csv'\n",
    "\n",
    "file_path = data_folder_path + file_name\n",
    "eval_set = pd.read_csv(file_path)\n",
    "\n",
    "display(eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate that the model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money does client 1 have in shares?\n",
      "\n",
      "Answer: Client 1 has £14,000 worth of NVDA shares (70% of 20 shares at £1000 per share) and £5,700 worth of AAPL shares (30% of 20 shares at £190 per share). Therefore, in total, Client 1 has £19,700 in shares.\n"
     ]
    }
   ],
   "source": [
    "question = eval_set['inputs'][0]\n",
    "context = eval_set['context'][0]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print('')\n",
    "print('Answer: ' + example_model.invoke({'inputs': question, 'context': context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect to ML-Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_llm(example_model, eval_set, \"openai:/gpt-3.5-turbo\", \"mlflow_development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['outputs'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the saved models using the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Specify the tracking URI (replace with your MLflow server URI)\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080/\")\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = client.get_experiment_by_name('mlflow_development')\n",
    "\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment 'mlflow_development' not found.\")\n",
    "\n",
    "# Fetch all runs for the experiment\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"\",  # Add filter criteria if needed\n",
    "    run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY  # You can choose ACTIVE_ONLY, DELETED_ONLY, or ALL\n",
    ")\n",
    "\n",
    "# Display information about each run\n",
    "for run in runs:\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Parameters: {run.data.params}\")\n",
    "    print(f\"Metrics: {run.data.metrics}\")\n",
    "    print(f\"Tags: {run.data.tags}\")\n",
    "    print(f\"Artifacts: {client.list_artifacts(run.info.run_id)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_experiment(experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/29 15:54:46 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/05/29 15:54:49 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/05/29 15:54:51 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/29 15:54:51 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/29 15:54:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/29 15:54:51 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/29 15:54:51 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/29 15:54:51 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "c:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "2024/05/29 15:54:57 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/29 15:54:57 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/29 15:54:57 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/29 15:54:57 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/29 15:54:57 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/29 15:54:57 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# from mlflow.metrics.genai import answer_similarity, faithfulness, answer_correctness, answer_relevance\n",
    "# judge_model = \"openai:/gpt-3.5-turbo\"\n",
    "\n",
    "# faithfulness_metric = faithfulness(model=judge_model)\n",
    "# answer_relevance_metric = answer_relevance(model=judge_model)\n",
    "# answer_similarity_metric = answer_similarity(model=judge_model)\n",
    "# answer_correctness_metric = answer_correctness(model=judge_model)\n",
    "\n",
    "# extra_metrics = [faithfulness_metric, answer_similarity_metric, answer_correctness_metric, answer_relevance_metric]\n",
    "\n",
    "# mlflow.set_experiment('mlflow_development')\n",
    "\n",
    "# with mlflow.start_run() as run: \n",
    "#     _logged_model = mlflow.langchain.log_model(example_model, artifact_path=\"model\")\n",
    "\n",
    "#     mlflow.log_param(\"model\", example_model)\n",
    "#     results = mlflow.evaluate(_logged_model.model_uri, eval_set, model_type=\"question-answering\",\n",
    "#                                 targets=\"targets\", extra_metrics=extra_metrics,\n",
    "#                                 evaluator_config={'col_mapping': {\"inputs\": \"predictions\"}})\n",
    "\n",
    "#     mlflow.log_metrics(results.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi working chat gpt Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.models.evaluation.base.EvaluationResult"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 283.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "      <th>answer_correctness/v1/score</th>\n",
       "      <th>answer_correctness/v1/justification</th>\n",
       "      <th>answer_relevance/v1/score</th>\n",
       "      <th>answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly states the distribution o...</td>\n",
       "      <td>3</td>\n",
       "      <td>The output has moderate semantic similarity to...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output provided by the model is mostly cor...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly mirrors the input, providi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "      <td>Client 2 has £7,300 in shares. This is calcula...</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly calculates the amount of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output aligns with the provided targets in...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provided by the model is correct an...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses all aspects of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...   \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...           66   \n",
       "1  Client 2 has £7,300 in shares. This is calcula...           76   \n",
       "\n",
       "   faithfulness/v1/score                      faithfulness/v1/justification  \\\n",
       "0                      5  The output correctly states the distribution o...   \n",
       "1                      5  The output correctly calculates the amount of ...   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           3   \n",
       "1                           4   \n",
       "\n",
       "                  answer_similarity/v1/justification  \\\n",
       "0  The output has moderate semantic similarity to...   \n",
       "1  The output aligns with the provided targets in...   \n",
       "\n",
       "   answer_correctness/v1/score  \\\n",
       "0                            4   \n",
       "1                            5   \n",
       "\n",
       "                 answer_correctness/v1/justification  \\\n",
       "0  The output provided by the model is mostly cor...   \n",
       "1  The output provided by the model is correct an...   \n",
       "\n",
       "   answer_relevance/v1/score  \\\n",
       "0                          5   \n",
       "1                          5   \n",
       "\n",
       "                   answer_relevance/v1/justification  \n",
       "0  The output directly mirrors the input, providi...  \n",
       "1  The output directly addresses all aspects of t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "#output_df['answer'] = [d['content'] for d in output_df['outputs']]\n",
    "\n",
    "# desired_columns = ['inputs', 'context', 'targets'] + [col for col in output_df.columns\n",
    "#                                             if ('score' in col) or ('justification') in col]\n",
    "# output_df = output_df[desired_columns]\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client 2 has £7,300 in shares. This is calculated by taking 30% of their shares in NVDA (3 shares x £1000 = £3000) and 70% of their shares in AAPL (7 shares x £190 = £1330), then adding these two amounts together (£3000 + £1330 = £4330).'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create score output func\n",
    "output_df['faithfulness/v1/score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 9f33508bd7124b8c9a8e154f79c2f707\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023F44913B10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000023F6643A810>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/p90': 4.9, 'answer_correctness/v1/variance': 0.25, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.5, 'answer_similarity/v1/p90': 3.9, 'answer_similarity/v1/variance': 0.25, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"129811ea46884d4cb26c39fe131fcdfc\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"9f33508bd7124b8c9a8e154f79c2f707\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 14:54:30.576005\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"129811ea46884d4cb26c39fe131fcdfc\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'tasteful-pig-749', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3495, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 60fa9c9e6a62439aa6e519cf738ea478\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D54A7FCD10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D54A637D10>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.log-model.history': '[{\"run_id\": \"60fa9c9e6a62439aa6e519cf738ea478\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 14:53:58.287493\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"f2080f56b138482f8f99302a929d5116\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.runName': 'abundant-snail-678', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: a8ad84686c2b4606b2f20b908abdeb52\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DCFBECBD50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DCFBED7690>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 4.0, 'answer_similarity/v1/p90': 4.0, 'answer_similarity/v1/variance': 0.0, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"e1f6e931e0b14b3f9967df52a87a4f0f\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"a8ad84686c2b4606b2f20b908abdeb52\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:30:26.049011\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"e1f6e931e0b14b3f9967df52a87a4f0f\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'defiant-fish-943', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3786, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 43c51047659d483381c6bf49a986df13\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"b2d6372e0ffe45f493876fecaa9a5cdb\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"43c51047659d483381c6bf49a986df13\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:28:24.004446\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"b2d6372e0ffe45f493876fecaa9a5cdb\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'upset-jay-389', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: c01bc47da2ea43fcb5eba9e33d20bc09\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.0, 'answer_similarity/v1/p90': 3.0, 'answer_similarity/v1/variance': 0.0, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0, 'relevance/v1/mean': 5.0, 'relevance/v1/p90': 5.0, 'relevance/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"8cf2b098c50f422f947a24ca6692f338\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"c01bc47da2ea43fcb5eba9e33d20bc09\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:26:41.719159\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"8cf2b098c50f422f947a24ca6692f338\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'indecisive-skink-399', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=1997, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: ce173b9dfb084da6b82828507c42f335\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"8f6183f00ba84ca0948949757aceda0c\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"ce173b9dfb084da6b82828507c42f335\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:26:16.509773\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"8f6183f00ba84ca0948949757aceda0c\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'unruly-elk-24', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 1e732c20df4b4f8aafdc614da6c97ab8\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.log-model.history': '[{\"run_id\": \"1e732c20df4b4f8aafdc614da6c97ab8\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:25:59.820766\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"32dd228a394e4c7282ac025a03a2b502\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'nebulous-doe-63', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading prompt is:\n",
      "\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "EvaluationMetric(name=Answer_Quality, greater_is_better=True, long_name=Answer_Quality, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called Answer_Quality based on the input and output.\n",
      "A definition of Answer_Quality and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer Quality is a measure of the accuracy of the answer.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is the best stock that client 2 currently owns?\n",
      "\n",
      "Example Output:\n",
      "The best performing stock owned by client 2 is NVDA, which has seen a 400% increase in value in the last 10 months.\n",
      "\n",
      "Example score: 80\n",
      "Example justification: The best performing stock has been identified, and a reason is given for its choosing.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "example = EvaluationExample(input=\"What is the best stock that client 2 currently owns?\",\n",
    "                            output=(r\"The best performing stock owned by client 2 is NVDA, which has seen a 400% \"\n",
    "                                    + \"increase in value in the last 10 months.\"),\n",
    "                            score=80,\n",
    "                            justification=(\"The best performing stock has been identified, and a reason is given for \"\n",
    "                                           + \"its choosing.\"))\n",
    "\n",
    "grading_prompt = (\"Answer Quality: If the answer given does not relate to the question, or if the question is not \"\n",
    "                  + \"answered, we will give a low score. If the question is answered comprehensively we will give a \"\n",
    "                  + \"higher score.\\nScore 0: The question is not answered.\\nScore 20: The question is barely \"\n",
    "                  + \"answered, and the answer is not useful.\\nScore 40: The question is barely answered in basic \"\n",
    "                  + \"terms.\\nScore 80: The question is barely answered correctly and accurately.\\nScore 100: The \"\n",
    "                  + \"question is answer perfectly, and the choices are well reasoned.\")\n",
    "\n",
    "# Make a metric from a Gen AI model.\n",
    "answer_quality = make_genai_metric(name=\"Answer_Quality\",\n",
    "                                   definition=(\"Answer Quality is a measure of the accuracy of the answer.\"),\n",
    "                                   model=\"openai:/gpt-3.5-turbo\",\n",
    "                                   examples=[example],\n",
    "                                   parameters={\"temperature\": 0.0},\n",
    "                                   aggregations=[\"mean\", \"variance\"],\n",
    "                                   greater_is_better=True,\n",
    "                                   grading_prompt=(grading_prompt))\n",
    "\n",
    "print('The grading prompt is:')\n",
    "print('')\n",
    "print(grading_prompt)\n",
    "print('')\n",
    "print(answer_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
