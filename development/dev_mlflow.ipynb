{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development: ML Flow\n",
    "\n",
    "File for developing the mlflow code for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Use the below lines if any dependencies are missing.\n",
    "# ! python -m pip install uv\n",
    "# ! python -m uv pip install langchain_openai mlflow load_dotenv langchain pandas langchain_community\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('\\\\'.join(os.getcwd().split('\\\\')[:-1])))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "from ml_flow import mlflow_server, create_example_llm, evaluate_llm, delete_runs\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we run the ML-Flow server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_process = mlflow_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a dummy LLM which will answer simple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                              target  \n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...  \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m First, we need to find out which client we are looking for and what type of shares they have.\n",
      "Action: [Search for client 1]\n",
      "Action Input: Client 1\u001b[0m[Search for client 1] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3mWe need to find a tool that can help us search for a specific client.\n",
      "Action: [Use a database or spreadsheet]\n",
      "Action Input: Client 1\u001b[0m[Use a database or spreadsheet] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3m We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a financial management software]\n",
      "Action Input: Client 1\u001b[0m[Use a financial management software] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3m We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a stock trading platform]\n",
      "Action Input: Client 1\u001b[0m[Use a stock trading platform] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3m We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a brokerage account]\n",
      "Action Input: Client 1\u001b[0m[Use a brokerage account] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Tool: [Search for client 1]\n",
      "Log:  First, we need to find out which client we are looking for and what type of shares they have.\n",
      "Action: [Search for client 1]\n",
      "Action Input: Client 1\n",
      "\n",
      "\n",
      "Tool: [Use a database or spreadsheet]\n",
      "Log: We need to find a tool that can help us search for a specific client.\n",
      "Action: [Use a database or spreadsheet]\n",
      "Action Input: Client 1\n",
      "\n",
      "\n",
      "Tool: [Use a financial management software]\n",
      "Log:  We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a financial management software]\n",
      "Action Input: Client 1\n",
      "\n",
      "\n",
      "Tool: [Use a stock trading platform]\n",
      "Log:  We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a stock trading platform]\n",
      "Action Input: Client 1\n",
      "\n",
      "\n",
      "Tool: [Use a brokerage account]\n",
      "Log:  We need to find a specific tool that can help us search for a specific client and their shares.\n",
      "Action: [Use a brokerage account]\n",
      "Action Input: Client 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We need to access the financial records of client 2 to find out their share holdings.\n",
      "Action: [Access financial records]\n",
      "Action Input: Client 2's financial records\u001b[0m[Access financial records] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3mWe can use a financial management software to access client 2's financial records.\n",
      "Action: [Use financial management software]\n",
      "Action Input: Client 2's financial records\u001b[0m[Use financial management software] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3mWe can ask the client directly for their share holdings.\n",
      "Action: [Ask client]\n",
      "Action Input: Client 2\u001b[0m[Ask client] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3mWe can check with the company's investor relations department for information on client 2's share holdings.\n",
      "Action: [Check with investor relations department]\n",
      "Action Input: Client 2's name\u001b[0m[Check with investor relations department] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3mWe can search for any public filings or reports that mention client 2's share holdings.\n",
      "Action: [Search public filings/reports]\n",
      "Action Input: Client 2's name\u001b[0m[Search public filings/reports] is not a valid tool, try one of [].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Tool: [Access financial records]\n",
      "Log:  We need to access the financial records of client 2 to find out their share holdings.\n",
      "Action: [Access financial records]\n",
      "Action Input: Client 2's financial records\n",
      "\n",
      "\n",
      "Tool: [Use financial management software]\n",
      "Log: We can use a financial management software to access client 2's financial records.\n",
      "Action: [Use financial management software]\n",
      "Action Input: Client 2's financial records\n",
      "\n",
      "\n",
      "Tool: [Ask client]\n",
      "Log: We can ask the client directly for their share holdings.\n",
      "Action: [Ask client]\n",
      "Action Input: Client 2\n",
      "\n",
      "\n",
      "Tool: [Check with investor relations department]\n",
      "Log: We can check with the company's investor relations department for information on client 2's share holdings.\n",
      "Action: [Check with investor relations department]\n",
      "Action Input: Client 2's name\n",
      "\n",
      "\n",
      "Tool: [Search public filings/reports]\n",
      "Log: We can search for any public filings or reports that mention client 2's share holdings.\n",
      "Action: [Search public filings/reports]\n",
      "Action Input: Client 2's name\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "import numpy as np\n",
    "from langchain import hub\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "def create_example_agent():\n",
    "    prompt = hub.pull(\"hwchase17/react\")\n",
    "    llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.2)\n",
    "\n",
    "    agent = create_react_agent(llm, tools=[], prompt=prompt)\n",
    "\n",
    "    return AgentExecutor(agent=agent, tools=[], handle_parsing_errors=True, verbose=True, max_iterations=5,\n",
    "                         return_intermediate_steps=True)\n",
    "\n",
    "example_agent = create_example_agent()\n",
    "\n",
    "data_folder_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) +'/data/'\n",
    "file_name = 'Evaluation Dataset.csv'\n",
    "\n",
    "file_path = data_folder_path + file_name\n",
    "eval_set = pd.read_csv(file_path)\n",
    "\n",
    "display(eval_set)\n",
    "\n",
    "def get_agent_thoughts(agent_result):\n",
    "\n",
    "    thoughts = []\n",
    "\n",
    "    for step in result['intermediate_steps']:\n",
    "        thoughts += [f'Tool: {step[0].tool}']\n",
    "        thoughts += [f'Log: {step[0].log}']\n",
    "        thoughts += '\\n'\n",
    "\n",
    "    return '\\n'.join(thoughts)\n",
    "\n",
    "all_score_values = {}\n",
    "\n",
    "for question in eval_set['question']:\n",
    "    result = example_agent.invoke({\"input\": question})\n",
    "    thoughts = get_agent_thoughts(result)\n",
    "\n",
    "    if 'ari_score' not in all_score_values.keys():\n",
    "        all_score_values['ari_score'] = []\n",
    "\n",
    "    all_score_values['ari_score'].append(textstat.automated_readability_index(result['output']))\n",
    "\n",
    "    print(thoughts)\n",
    "\n",
    "experiment_name = 'mlflow_development'\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "for score_name, score_values in all_score_values.items():\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metric(score_name + '_mean', np.mean(score_values))\n",
    "        mlflow.log_metric(score_name + '_variance', np.var(score_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                             targets  \n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...  \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "     -------------------------------------- 105.1/105.1 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 8.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.15.0 textstat-0.7.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableAgent' object has no attribute 'llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment_name)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m eval_set\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 16\u001b[0m     _logged_model \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlangchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent_executor)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[1;32mc:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\mlflow\\langchain\\__init__.py:542\u001b[0m, in \u001b[0;36mlog_model\u001b[1;34m(lc_model, artifact_path, conda_env, code_paths, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, loader_fn, persist_dir, example_no_conversion, run_id, model_config)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;129m@experimental\u001b[39m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;129m@format_docstring\u001b[39m(LOG_MODEL_PARAM_DOCS\u001b[38;5;241m.\u001b[39mformat(package_name\u001b[38;5;241m=\u001b[39mFLAVOR_NAME))\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_model\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    428\u001b[0m     model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    429\u001b[0m ):\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    Log a LangChain model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m        metadata of the logged model.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     lc_model_or_path \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_and_prepare_lc_model_or_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m    545\u001b[0m         artifact_path\u001b[38;5;241m=\u001b[39martifact_path,\n\u001b[0;32m    546\u001b[0m         flavor\u001b[38;5;241m=\u001b[39mmlflow\u001b[38;5;241m.\u001b[39mlangchain,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    561\u001b[0m         model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[0;32m    562\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\mlflow\\langchain\\utils.py:316\u001b[0m, in \u001b[0;36m_validate_and_prepare_lc_model_or_path\u001b[1;34m(lc_model, loader_fn)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lc_model, langchain\u001b[38;5;241m.\u001b[39mchains\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mLLMChain) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39mllm, supported_llm) \u001b[38;5;28;01mfor\u001b[39;00m supported_llm \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_LLMS\n\u001b[0;32m    310\u001b[0m ):\n\u001b[0;32m    311\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    312\u001b[0m         _UNSUPPORTED_LLM_WARNING_MESSAGE,\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;28mtype\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39mllm)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    314\u001b[0m     )\n\u001b[1;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lc_model, langchain\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mAgentExecutor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupported_llm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_llm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_SUPPORTED_LLMS\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    319\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    320\u001b[0m         _UNSUPPORTED_LLM_WARNING_MESSAGE,\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28mtype\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mllm)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m special_chain_info \u001b[38;5;241m:=\u001b[39m _get_special_chain_info_or_none(lc_model):\n",
      "File \u001b[1;32mc:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\mlflow\\langchain\\utils.py:317\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lc_model, langchain\u001b[38;5;241m.\u001b[39mchains\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mLLMChain) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39mllm, supported_llm) \u001b[38;5;28;01mfor\u001b[39;00m supported_llm \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_LLMS\n\u001b[0;32m    310\u001b[0m ):\n\u001b[0;32m    311\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    312\u001b[0m         _UNSUPPORTED_LLM_WARNING_MESSAGE,\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;28mtype\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39mllm)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    314\u001b[0m     )\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lc_model, langchain\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mAgentExecutor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[43mlc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241m.\u001b[39mllm, supported_llm) \u001b[38;5;28;01mfor\u001b[39;00m supported_llm \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_LLMS\n\u001b[0;32m    318\u001b[0m ):\n\u001b[0;32m    319\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    320\u001b[0m         _UNSUPPORTED_LLM_WARNING_MESSAGE,\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28mtype\u001b[39m(lc_model\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mllm)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m special_chain_info \u001b[38;5;241m:=\u001b[39m _get_special_chain_info_or_none(lc_model):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableAgent' object has no attribute 'llm_chain'"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import answer_similarity, faithfulness, answer_correctness, answer_relevance\n",
    "\n",
    "judge_model = \"openai:/gpt-3.5-turbo\"\n",
    "experiment_name = 'mlflow_development'\n",
    "\n",
    "# faithfulness_metric = faithfulness(model=judge_model)\n",
    "answer_relevance_metric = answer_relevance(model=judge_model)\n",
    "# answer_similarity_metric = answer_similarity(model=judge_model)\n",
    "# answer_correctness_metric = answer_correctness(model=judge_model)\n",
    "\n",
    "# extra_metrics = [faithfulness_metric, answer_similarity_metric, answer_correctness_metric, answer_relevance_metric]\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "#log mean and variance of all metrics \n",
    "\n",
    "for i, row in eval_set.iterrows():\n",
    "    # _logged_model = mlflow.langchain.log_model(agent_executor, artifact_path=\"model\")\n",
    "    # mlflow.log_param(\"model\", agent_executor)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    \n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metric(\"ARI_grade_level\", ari_score)\n",
    "\n",
    "\n",
    "# with mlflow.start_run() as run: \n",
    "#     _logged_model = mlflow.langchain.log_model(llm_to_evaluate, artifact_path=\"model\")\n",
    "\n",
    "#     mlflow.log_param(\"model\", llm_to_evaluate)\n",
    "#     results = mlflow.evaluate(_logged_model.model_uri, evaluation_dataset, model_type=\"question-answering\",\n",
    "#                                 targets=\"targets\", extra_metrics=extra_metrics,\n",
    "#                                 evaluator_config={'col_mapping': {\"inputs\": \"predictions\"}})\n",
    "\n",
    "#     mlflow.log_metrics(results.metrics)\n",
    "\n",
    "# return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then read in an evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                             targets  \n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...  \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) +'/data/'\n",
    "file_name = 'Evaluation Dataset.csv'\n",
    "\n",
    "file_path = data_folder_path + file_name\n",
    "eval_set = pd.read_csv(file_path)\n",
    "\n",
    "display(eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate that the model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money does client 1 have in shares?\n",
      "\n",
      "Answer: Client 1 has £14,000 worth of NVDA shares (70% of 20 shares at £1000 per share) and £5,700 worth of APPL shares (30% of 20 shares at £190 per share). Therefore, in total, Client 1 has £19,700 in shares.\n"
     ]
    }
   ],
   "source": [
    "question = eval_set['inputs'][0]\n",
    "context = eval_set['context'][0]\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print('')\n",
    "print('Answer: ' + example_model.invoke({'inputs': question, 'context': context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then connect to ML-Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/30 09:25:36 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/05/30 09:25:38 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/05/30 09:25:40 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/30 09:25:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/30 09:25:40 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/30 09:25:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/30 09:25:40 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/30 09:25:40 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "c:\\Code\\GenAIGroupProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "2024/05/30 09:25:44 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/30 09:25:44 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/30 09:25:44 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for flesch kincaid metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/30 09:25:44 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/30 09:25:44 WARNING mlflow.metrics.metric_definitions: Failed to import textstat for automated readability index metric, skipping metric logging. Please install textstat using 'pip install textstat'.\n",
      "2024/05/30 09:25:44 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.38it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_llm(example_model, eval_set, \"openai:/gpt-3.5-turbo\", \"mlflow_development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 742.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>context</th>\n",
       "      <th>targets</th>\n",
       "      <th>outputs</th>\n",
       "      <th>token_count</th>\n",
       "      <th>faithfulness/v1/score</th>\n",
       "      <th>faithfulness/v1/justification</th>\n",
       "      <th>answer_similarity/v1/score</th>\n",
       "      <th>answer_similarity/v1/justification</th>\n",
       "      <th>answer_correctness/v1/score</th>\n",
       "      <th>answer_correctness/v1/justification</th>\n",
       "      <th>answer_relevance/v1/score</th>\n",
       "      <th>answer_relevance/v1/justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much money does client 1 have in shares?</td>\n",
       "      <td>Client 1 has 20 shares. 70% of their shares ar...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>Client 1 has £14,000 worth of NVDA shares (70%...</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly states the distribution o...</td>\n",
       "      <td>3</td>\n",
       "      <td>The output has moderate semantic similarity to...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output is correct and demonstrates a high ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output provided directly addresses all asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much money does client 2 have in shares?</td>\n",
       "      <td>Client 2 has 10 shares. 30% of their shares ar...</td>\n",
       "      <td>Client 2 has £3,000 worth of NVDA shares (30% ...</td>\n",
       "      <td>Client 2 has £7,300 in shares. This is calcula...</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>The output correctly calculates the amount of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The output aligns with the provided targets in...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output is correct and demonstrates a high ...</td>\n",
       "      <td>5</td>\n",
       "      <td>The output directly addresses all aspects of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs  \\\n",
       "0  How much money does client 1 have in shares?   \n",
       "1  How much money does client 2 have in shares?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Client 1 has 20 shares. 70% of their shares ar...   \n",
       "1  Client 2 has 10 shares. 30% of their shares ar...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...   \n",
       "1  Client 2 has £3,000 worth of NVDA shares (30% ...   \n",
       "\n",
       "                                             outputs  token_count  \\\n",
       "0  Client 1 has £14,000 worth of NVDA shares (70%...           66   \n",
       "1  Client 2 has £7,300 in shares. This is calcula...           76   \n",
       "\n",
       "   faithfulness/v1/score                      faithfulness/v1/justification  \\\n",
       "0                      5  The output correctly states the distribution o...   \n",
       "1                      5  The output correctly calculates the amount of ...   \n",
       "\n",
       "   answer_similarity/v1/score  \\\n",
       "0                           3   \n",
       "1                           4   \n",
       "\n",
       "                  answer_similarity/v1/justification  \\\n",
       "0  The output has moderate semantic similarity to...   \n",
       "1  The output aligns with the provided targets in...   \n",
       "\n",
       "   answer_correctness/v1/score  \\\n",
       "0                            5   \n",
       "1                            5   \n",
       "\n",
       "                 answer_correctness/v1/justification  \\\n",
       "0  The output is correct and demonstrates a high ...   \n",
       "1  The output is correct and demonstrates a high ...   \n",
       "\n",
       "   answer_relevance/v1/score  \\\n",
       "0                          5   \n",
       "1                          5   \n",
       "\n",
       "                   answer_relevance/v1/justification  \n",
       "0  The output provided directly addresses all asp...  \n",
       "1  The output directly addresses all aspects of t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Client 2 has £7,300 in shares. This is calculated by taking 30% of their shares in NVDA (3 shares x £1000 = £3000) and 70% of their shares in AAPL (7 shares x £190 = £1330), then adding these two amounts together (£3000 + £1330 = £4330).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df['outputs'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the saved models using the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 58a8e7cbb25e43faa27d4a17738836e5\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002C69B59DB50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002C6F9CFA390>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.5, 'answer_similarity/v1/p90': 3.9, 'answer_similarity/v1/variance': 0.25, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"d0e14892ef474e72b443a90f954ba503\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"58a8e7cbb25e43faa27d4a17738836e5\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-30 08:25:21.129388\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"d0e14892ef474e72b443a90f954ba503\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'hilarious-fly-936', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3540, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 7ff27b9287f54908a8509d29a7fd98bc\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000021137F5D610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000021137F54690>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 4.0, 'answer_similarity/v1/p90': 4.8, 'answer_similarity/v1/variance': 1.0, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"5f232475253f49599a820ab1c26b9575\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"7ff27b9287f54908a8509d29a7fd98bc\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-30 08:20:54.366749\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"5f232475253f49599a820ab1c26b9575\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'useful-fawn-581', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3673, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 264f06df78e9418ba85316cbd8dba4b2\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002112E7ABB10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002112E7C5AD0>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/p90': 4.9, 'answer_correctness/v1/variance': 0.25, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.5, 'answer_similarity/v1/p90': 3.9, 'answer_similarity/v1/variance': 0.25, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"6705872e4f7141e4abf10dd3a4212e92\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"264f06df78e9418ba85316cbd8dba4b2\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 14:59:41.045018\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"6705872e4f7141e4abf10dd3a4212e92\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'marvelous-moose-922', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3473, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 9f33508bd7124b8c9a8e154f79c2f707\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023F44913B10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000023F6643A810>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 4.5, 'answer_correctness/v1/p90': 4.9, 'answer_correctness/v1/variance': 0.25, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.5, 'answer_similarity/v1/p90': 3.9, 'answer_similarity/v1/variance': 0.25, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"129811ea46884d4cb26c39fe131fcdfc\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"9f33508bd7124b8c9a8e154f79c2f707\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 14:54:30.576005\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"129811ea46884d4cb26c39fe131fcdfc\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'tasteful-pig-749', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3495, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 60fa9c9e6a62439aa6e519cf738ea478\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D54A7FCD10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D54A637D10>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.log-model.history': '[{\"run_id\": \"60fa9c9e6a62439aa6e519cf738ea478\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 14:53:58.287493\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"f2080f56b138482f8f99302a929d5116\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1393}]', 'mlflow.runName': 'abundant-snail-678', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: a8ad84686c2b4606b2f20b908abdeb52\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DCFBECBD50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DCFBED7690>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 4.0, 'answer_similarity/v1/p90': 4.0, 'answer_similarity/v1/variance': 0.0, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"f4b6912a1feeee0b23e37f229253ecab\",\"hash\":\"f4b6912a1feeee0b23e37f229253ecab\",\"model\":\"e1f6e931e0b14b3f9967df52a87a4f0f\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"a8ad84686c2b4606b2f20b908abdeb52\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:30:26.049011\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"e1f6e931e0b14b3f9967df52a87a4f0f\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'defiant-fish-943', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=3786, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 43c51047659d483381c6bf49a986df13\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"b2d6372e0ffe45f493876fecaa9a5cdb\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"43c51047659d483381c6bf49a986df13\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:28:24.004446\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"b2d6372e0ffe45f493876fecaa9a5cdb\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'upset-jay-389', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: c01bc47da2ea43fcb5eba9e33d20bc09\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {'answer_correctness/v1/mean': 5.0, 'answer_correctness/v1/p90': 5.0, 'answer_correctness/v1/variance': 0.0, 'answer_relevance/v1/mean': 5.0, 'answer_relevance/v1/p90': 5.0, 'answer_relevance/v1/variance': 0.0, 'answer_similarity/v1/mean': 3.0, 'answer_similarity/v1/p90': 3.0, 'answer_similarity/v1/variance': 0.0, 'exact_match/v1': 0.0, 'faithfulness/v1/mean': 5.0, 'faithfulness/v1/p90': 5.0, 'faithfulness/v1/variance': 0.0, 'relevance/v1/mean': 5.0, 'relevance/v1/p90': 5.0, 'relevance/v1/variance': 0.0}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"8cf2b098c50f422f947a24ca6692f338\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"c01bc47da2ea43fcb5eba9e33d20bc09\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:26:41.719159\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"8cf2b098c50f422f947a24ca6692f338\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.loggedArtifacts': '[{\"path\": \"eval_results_table.json\", \"type\": \"table\"}]', 'mlflow.runName': 'indecisive-skink-399', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=1997, is_dir=False, path='eval_results_table.json'>, <FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: ce173b9dfb084da6b82828507c42f335\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.datasets': '[{\"name\":\"a701eccb1269c49d6f3a64e53354696c\",\"hash\":\"a701eccb1269c49d6f3a64e53354696c\",\"model\":\"8f6183f00ba84ca0948949757aceda0c\"}]', 'mlflow.log-model.history': '[{\"run_id\": \"ce173b9dfb084da6b82828507c42f335\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:26:16.509773\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"8f6183f00ba84ca0948949757aceda0c\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'unruly-elk-24', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n",
      "Run ID: 1e732c20df4b4f8aafdc614da6c97ab8\n",
      "Parameters: {'model': 'first=PromptTemplate(input_variables=[\\'context\\', \\'inputs\\'], template=\"You\\'re a investment manager. Using the context provided, reply to the question below to the best of your ability:\\\\nQuestion:\\\\n{inputs}\\\\nContext:\\\\n{context}\") middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000015E504CBDD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000015E502B7210>, model_name=\\'gpt-3.5-turbo-0125\\', temperature=0.0, openai_api_key=SecretStr(\\'**********\\'), openai_proxy=\\'\\')] last=RunnableLambda(_get_content)'}\n",
      "Metrics: {}\n",
      "Tags: {'mlflow.log-model.history': '[{\"run_id\": \"1e732c20df4b4f8aafdc614da6c97ab8\", \"artifact_path\": \"model\", \"utc_time_created\": \"2024-05-29 13:25:59.820766\", \"flavors\": {\"python_function\": {\"predict_stream_fn\": \"predict_stream\", \"streamable\": true, \"model_load\": \"runnable_load\", \"model_type\": \"RunnableSequence\", \"model_data\": \"model\", \"loader_module\": \"mlflow.langchain\", \"python_version\": \"3.11.0\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"langchain\": {\"langchain_version\": \"0.2.1\", \"code\": null, \"streamable\": true, \"model_type\": \"RunnableSequence\", \"model_load\": \"runnable_load\", \"model_data\": \"model\"}}, \"model_uuid\": \"32dd228a394e4c7282ac025a03a2b502\", \"mlflow_version\": \"2.13.0\", \"model_size_bytes\": 1371}]', 'mlflow.runName': 'nebulous-doe-63', 'mlflow.source.name': 'c:\\\\Code\\\\GenAIGroupProject\\\\.venv\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.user': 'MichaelBerney'}\n",
      "Artifacts: [<FileInfo: file_size=None, is_dir=True, path='model'>]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080/\")\n",
    "client = MlflowClient()\n",
    "\n",
    "experiment = client.get_experiment_by_name('mlflow_development')\n",
    "\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment 'mlflow_development' not found.\")\n",
    "\n",
    "runs = client.search_runs(experiment_ids=[experiment.experiment_id],\n",
    "                          filter_string=\"\",  # Add filter criteria if needed\n",
    "                          run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY)\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Parameters: {run.data.params}\")\n",
    "    print(f\"Metrics: {run.data.metrics}\")\n",
    "    print(f\"Tags: {run.data.tags}\")\n",
    "    print(f\"Artifacts: {client.list_artifacts(run.info.run_id)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Code\n",
    "\n",
    "Other code used in the development of this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlflow.metrics.genai import answer_similarity, faithfulness, answer_correctness, answer_relevance\n",
    "# judge_model = \"openai:/gpt-3.5-turbo\"\n",
    "\n",
    "# faithfulness_metric = faithfulness(model=judge_model)\n",
    "# answer_relevance_metric = answer_relevance(model=judge_model)\n",
    "# answer_similarity_metric = answer_similarity(model=judge_model)\n",
    "# answer_correctness_metric = answer_correctness(model=judge_model)\n",
    "\n",
    "# extra_metrics = [faithfulness_metric, answer_similarity_metric, answer_correctness_metric, answer_relevance_metric]\n",
    "\n",
    "# mlflow.set_experiment('mlflow_development')\n",
    "\n",
    "# with mlflow.start_run() as run: \n",
    "#     _logged_model = mlflow.langchain.log_model(example_model, artifact_path=\"model\")\n",
    "\n",
    "#     mlflow.log_param(\"model\", example_model)\n",
    "#     results = mlflow.evaluate(_logged_model.model_uri, eval_set, model_type=\"question-answering\",\n",
    "#                                 targets=\"targets\", extra_metrics=extra_metrics,\n",
    "#                                 evaluator_config={'col_mapping': {\"inputs\": \"predictions\"}})\n",
    "\n",
    "#     mlflow.log_metrics(results.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semi working chat gpt Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.models.evaluation.base.EvaluationResult"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/30 09:25:53 WARNING mlflow.tracking.client: Not all runs have the specified table artifact. Some runs will be skipped.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_results_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_results_table\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#output_df['answer'] = [d['content'] for d in output_df['outputs']]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# desired_columns = ['inputs', 'context', 'targets'] + [col for col in output_df.columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#                                             if ('score' in col) or ('justification') in col]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# output_df = output_df[desired_columns]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m display(output_df)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_results_table'"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results.tables['eval_results_table'])\n",
    "#output_df['answer'] = [d['content'] for d in output_df['outputs']]\n",
    "\n",
    "# desired_columns = ['inputs', 'context', 'targets'] + [col for col in output_df.columns\n",
    "#                                             if ('score' in col) or ('justification') in col]\n",
    "# output_df = output_df[desired_columns]\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create score output func\n",
    "output_df['faithfulness/v1/score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading prompt is:\n",
      "\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "EvaluationMetric(name=Answer_Quality, greater_is_better=True, long_name=Answer_Quality, version=v1, metric_details=\n",
      "Task:\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "You are an impartial judge. You will be given an input that was sent to a machine\n",
      "learning model, and you will be given an output that the model produced. You\n",
      "may also be given additional information that was used by the model to generate the output.\n",
      "\n",
      "Your task is to determine a numerical score called Answer_Quality based on the input and output.\n",
      "A definition of Answer_Quality and a grading rubric are provided below.\n",
      "You must use the grading rubric to determine your score. You must also justify your score.\n",
      "\n",
      "Examples could be included below for reference. Make sure to use them as references and to\n",
      "understand them before completing the task.\n",
      "\n",
      "Input:\n",
      "{input}\n",
      "\n",
      "Output:\n",
      "{output}\n",
      "\n",
      "{grading_context_columns}\n",
      "\n",
      "Metric definition:\n",
      "Answer Quality is a measure of the accuracy of the answer.\n",
      "\n",
      "Grading rubric:\n",
      "Answer Quality: If the answer given does not relate to the question, or if the question is not answered, we will give a low score. If the question is answered comprehensively we will give a higher score.\n",
      "Score 0: The question is not answered.\n",
      "Score 20: The question is barely answered, and the answer is not useful.\n",
      "Score 40: The question is barely answered in basic terms.\n",
      "Score 80: The question is barely answered correctly and accurately.\n",
      "Score 100: The question is answer perfectly, and the choices are well reasoned.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Example Input:\n",
      "What is the best stock that client 2 currently owns?\n",
      "\n",
      "Example Output:\n",
      "The best performing stock owned by client 2 is NVDA, which has seen a 400% increase in value in the last 10 months.\n",
      "\n",
      "Example score: 80\n",
      "Example justification: The best performing stock has been identified, and a reason is given for its choosing.\n",
      "        \n",
      "\n",
      "You must return the following fields in your response in two lines, one below the other:\n",
      "score: Your numerical score for the model's Answer_Quality based on the rubric\n",
      "justification: Your reasoning about the model's Answer_Quality score\n",
      "\n",
      "Do not add additional new lines. Do not add any other fields.\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "example = EvaluationExample(input=\"What is the best stock that client 2 currently owns?\",\n",
    "                            output=(r\"The best performing stock owned by client 2 is NVDA, which has seen a 400% \"\n",
    "                                    + \"increase in value in the last 10 months.\"),\n",
    "                            score=80,\n",
    "                            justification=(\"The best performing stock has been identified, and a reason is given for \"\n",
    "                                           + \"its choosing.\"))\n",
    "\n",
    "grading_prompt = (\"Answer Quality: If the answer given does not relate to the question, or if the question is not \"\n",
    "                  + \"answered, we will give a low score. If the question is answered comprehensively we will give a \"\n",
    "                  + \"higher score.\\nScore 0: The question is not answered.\\nScore 20: The question is barely \"\n",
    "                  + \"answered, and the answer is not useful.\\nScore 40: The question is barely answered in basic \"\n",
    "                  + \"terms.\\nScore 80: The question is barely answered correctly and accurately.\\nScore 100: The \"\n",
    "                  + \"question is answer perfectly, and the choices are well reasoned.\")\n",
    "\n",
    "# Make a metric from a Gen AI model.\n",
    "answer_quality = make_genai_metric(name=\"Answer_Quality\",\n",
    "                                   definition=(\"Answer Quality is a measure of the accuracy of the answer.\"),\n",
    "                                   model=\"openai:/gpt-3.5-turbo\",\n",
    "                                   examples=[example],\n",
    "                                   parameters={\"temperature\": 0.0},\n",
    "                                   aggregations=[\"mean\", \"variance\"],\n",
    "                                   greater_is_better=True,\n",
    "                                   grading_prompt=(grading_prompt))\n",
    "\n",
    "print('The grading prompt is:')\n",
    "print('')\n",
    "print(grading_prompt)\n",
    "print('')\n",
    "print(answer_quality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
